# ğŸ¤Ÿ Smart Glasses â€” Indian Sign Language (ISL) to Speech

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)

Wearable smart glasses with integrated camera that recognize Indian Sign Language (ISL) in real-time, convert signs to text, then to speech audio via Bluetooth speaker pendant. Helps mute/deaf ISL users communicate seamlessly with hearing people.

![Smart Glasses Demo](docs/images/demo.gif)

---

## ğŸ“– Table of Contents

- [How It Works](#how-it-works)
- [Features](#features)
- [Tech Stack](#tech-stack)
- [Hardware Requirements](#hardware-requirements)
- [Project Structure](#project-structure)
- [Quick Start](#quick-start)
- [Hardware Assembly](#hardware-assembly)
- [ISL Datasets](#isl-datasets)
- [Training Your Own Model](#training-your-own-model)
- [Mobile Web App](#mobile-web-app)
- [Deployment](#deployment)
- [Roadmap](#roadmap)
- [Contributing](#contributing)
- [References](#references)
- [License](#license)

---

## ğŸ”„ How It Works

Complete pipeline from camera to speech:

```
ğŸ“· Camera Capture (640Ã—480 @ 30fps)
    â†“
ğŸ‘ï¸ MediaPipe Hands Detection (21 landmarks Ã— 3 coords Ã— 2 hands = 126 features)
    â†“
ğŸ§  Neural Network Classifier (Dense layers with BatchNorm & Dropout)
    â†“
ğŸ“ ISL Sign Text (with confidence threshold & stability buffer)
    â†“
ğŸ”Š pyttsx3 Text-to-Speech (espeak backend on Pi)
    â†“
ğŸ”µ Bluetooth Speaker Output (via PulseAudio)
```

### Technical Details

1. **Hand Detection**: MediaPipe Hands extracts 21 3D landmarks per hand (x, y, z coordinates)
2. **Feature Extraction**: 126 features total (2 hands Ã— 21 landmarks Ã— 3 coords), normalized relative to wrist
3. **Classification**: Deep neural network with 256â†’128â†’64 neurons + regularization
4. **Stability Filter**: Requires 15 consecutive identical predictions before announcing
5. **Cooldown Timer**: Prevents repeated announcements within 3 seconds
6. **TTS Output**: Runs in separate thread to avoid blocking video processing
7. **Bluetooth Audio**: PulseAudio routes speech to paired BT speaker

---

## âœ¨ Features

- âœ… **Real-time ISL Recognition** â€” 15-20 FPS on Raspberry Pi Zero 2W
- âœ… **Mobile Web Control Panel** â€” Pair Bluetooth, configure settings, collect data via phone browser
- âœ… **No Command Line Pairing** â€” Everything controlled through webapp
- âœ… **Confidence Thresholding** â€” Only announces high-confidence predictions (default 70%)
- âœ… **Stability Buffer** â€” Reduces false positives with consecutive frame verification
- âœ… **Customizable Training** â€” Collect your own ISL signs with included data collection tool
- âœ… **TensorFlow Lite Optimization** â€” Fast inference on resource-constrained devices
- âœ… **3D Printable Frame** â€” Complete design specifications included
- âœ… **Battery Powered** â€” 2-3 hour runtime with 2000mAh LiPo
- âœ… **Bluetooth Speaker Pendant** â€” Clean audio without glasses vibration

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **SBC** | Raspberry Pi Zero 2W | Main processing unit (quad-core ARM) |
| **Camera** | OV5647 5MP Nano Module | Video capture via CSI interface |
| **Computer Vision** | MediaPipe Hands | Hand landmark detection |
| **ML Framework** | TensorFlow 2.16 + Keras | Model training |
| **Inference** | TensorFlow Lite | Optimized on-device inference |
| **TTS** | pyttsx3 + espeak | Text-to-speech conversion |
| **Web Framework** | Flask 3.0 + Flask-SocketIO | Mobile web interface |
| **Audio Output** | PulseAudio + Bluetooth | Wireless audio to speaker |
| **Languages** | Python 3.9+ | Primary language |
| **Dependencies** | OpenCV, NumPy, scikit-learn | Image processing & ML utilities |

---

## ğŸ”§ Hardware Requirements

### Bill of Materials (BOM) â€” Budget India Prices

| Component | Model/Specs | Price (INR) | Where to Buy | Why This One |
|-----------|------------|-------------|--------------|--------------|
| **SBC** | Raspberry Pi Zero 2 W (65Ã—30Ã—5mm, 11g) | â‚¹1,800â€“2,200 | [Robu.in](https://robu.in), ThinkRobotics | Quad-core ARM, WiFi+BT, tiny form factor, runs TF Lite |
| **Camera** | OV5647 Mini 5MP Nano (8.5Ã—8.5Ã—7mm) | â‚¹350â€“500 | [Amazon.in](https://amazon.in), Robu.in | Ultra-compact nano module fits in glasses bridge, 1080p, CSI |
| **CSI Cable** | 15cm FFC 22pin-to-15pin adapter | â‚¹80â€“150 | Amazon.in | Essential for Pi Zero mini CSI connector |
| **MicroSD** | SanDisk 32GB Class 10 A1 | â‚¹350â€“450 | Amazon.in | Fast boot + model loading, reliable brand |
| **Battery** | 3.7V 2000mAh LiPo (60Ã—35Ã—8mm slim) | â‚¹250â€“400 | Robu.in | 2-3hr runtime, lightweight, slim form factor |
| **Charger** | TP4056 USB-C Module (25Ã—19Ã—1mm) | â‚¹50â€“80 | Robu.in | Safe LiPo charging with overcharge protection |
| **Boost Converter** | MT3608 Step-Up 3.7Vâ†’5V (36Ã—17Ã—14mm) | â‚¹40â€“70 | Robu.in | Regulated 5V for Pi from LiPo battery |
| **Speaker** | BT 5.0 Pendant Speaker (any small BT) | â‚¹250â€“400 | Amazon.in | Separate pendant = no vibration on glasses, better audio |
| **Power Switch** | SPDT Slide Switch SS12D00 (7Ã—3Ã—3mm) | â‚¹10â€“20 | Robu.in | Manual on/off control |
| **Capacitor** | Electrolytic 10ÂµF 16V | â‚¹5â€“10 | Robu.in | Smooth power output from boost converter |
| **Misc** | Jumper wires, JST 2-pin, heat shrink | â‚¹100â€“200 | Robu.in | Assembly and wiring |
| **Glasses Frame** | 3D Printed (PETG/ABS) | â‚¹100â€“300 | Own 3D printer | Custom design houses all components |
| | **TOTAL** | **â‚¹3,385â€“4,780** | | Complete working system |

### Speaker Placement Philosophy

**Version 1 (Current)**: Bluetooth pendant/neckband speaker
- âœ… No vibration affecting camera stability
- âœ… Better audio quality (larger speaker)
- âœ… Easy to replace/upgrade
- âœ… No additional weight on glasses
- âœ… User can position for optimal hearing

**Version 2 (Future)**: Bone conduction in temple arms
- Would require custom bone conduction transducers
- More complex integration
- Recommended only after V1 is working perfectly

---

## ğŸ“ Project Structure

```
Smart_Glasses/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ collect_data.py      # Data collection script with MediaPipe
â”‚   â”œâ”€â”€ train_model.py        # Model training with TensorFlow/Keras
â”‚   â”œâ”€â”€ inference.py          # Real-time inference with TTS (desktop)
â”‚   â”œâ”€â”€ deploy_pi.py          # Raspberry Pi deployment (TFLite)
â”‚   â””â”€â”€ utils.py              # Utility functions (landmarks, models)
â”œâ”€â”€ webapp/
â”‚   â”œâ”€â”€ app.py                # Flask web app for mobile control
â”‚   â”œâ”€â”€ templates/            # HTML templates
â”‚   â”‚   â”œâ”€â”€ base.html         # Base template with navigation
â”‚   â”‚   â”œâ”€â”€ index.html        # Dashboard (status, start/stop)
â”‚   â”‚   â”œâ”€â”€ bluetooth.html    # Bluetooth pairing interface
â”‚   â”‚   â”œâ”€â”€ settings.html     # Configuration settings
â”‚   â”‚   â”œâ”€â”€ collect.html      # Data collection control
â”‚   â”‚   â”œâ”€â”€ train.html        # Training control
â”‚   â”‚   â””â”€â”€ logs.html         # Live logs viewer
â”‚   â””â”€â”€ static/
â”‚       â””â”€â”€ style.css         # Mobile-responsive dark theme CSS
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ HARDWARE_GUIDE.md     # Detailed hardware assembly guide
â”‚   â”œâ”€â”€ ML_TRAINING_GUIDE.md  # Beginner-friendly ML guide
â”‚   â””â”€â”€ WEBAPP_GUIDE.md       # Web app usage guide
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_pi.sh           # Complete Pi setup script
â”‚   â””â”€â”€ start_glasses.sh      # Start script for deployment
â”œâ”€â”€ models/                    # Trained models (gitignored)
â”‚   â”œâ”€â”€ isl_model.keras       # Full Keras model
â”‚   â”œâ”€â”€ isl_model.tflite      # TFLite model for Pi
â”‚   â”œâ”€â”€ labels.npy            # Label list
â”‚   â””â”€â”€ training_plot.png     # Training curves
â”œâ”€â”€ data/                      # Training data CSVs (gitignored)
â”‚   â””â”€â”€ SignName.csv          # One CSV per sign class
â”œâ”€â”€ logs/                      # Application logs (gitignored)
â”‚   â””â”€â”€ smart_glasses.log     # Main log file
â”œâ”€â”€ config.json               # Configuration file
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .gitignore               # Git ignore patterns
â”œâ”€â”€ LICENSE                  # MIT License
â””â”€â”€ README.md               # This file
```

---

## ğŸš€ Quick Start

### 1. Clone Repository

```bash
git clone https://github.com/Tusharjain-19/Smart_Glasses.git
cd Smart_Glasses
```

### 2. Setup Python Environment

```bash
# Create virtual environment
python3 -m venv venv

# Activate virtual environment
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### 3. Collect Training Data

Collect samples for each ISL sign you want to recognize:

```bash
python src/collect_data.py --sign "Hello" --samples 200
python src/collect_data.py --sign "Thanks" --samples 200
python src/collect_data.py --sign "A" --samples 200
# ... collect more signs
```

**Instructions during collection:**
- Press **S** to start/stop collecting
- Show the sign clearly to webcam
- Vary angles, distances, lighting
- Press **Q** to quit

### 4. Train Model

```bash
python src/train_model.py
```

This will:
- Load all collected CSVs from `data/`
- Train a neural network
- Save model to `models/isl_model.keras`
- Save TFLite model to `models/isl_model.tflite`
- Save labels to `models/labels.npy`
- Generate training plot

### 5. Test Inference (Desktop)

```bash
python src/inference.py
```

**Controls:**
- Show ISL signs to webcam
- Press **R** to reset stability buffer
- Press **Q** to quit

### 6. Deploy to Raspberry Pi

See [Deployment](#deployment) section below.

---

## ğŸ”© Hardware Assembly

See detailed guide: [docs/HARDWARE_GUIDE.md](docs/HARDWARE_GUIDE.md)

**Quick Overview:**

1. **3D Print Glasses Frame** (PETG, 0.15mm layers, 50% infill)
2. **Mount Camera** in bridge recess, route CSI cable to right temple
3. **Install Pi Zero** in right temple cavity
4. **Wire Power Circuit** in left temple:
   - LiPo â†’ TP4056 (charging) â†’ MT3608 (5V boost) â†’ Switch â†’ Pi
5. **Connect Components** via GPIO
6. **Close Access Panels** (snap-fit)
7. **Pair Bluetooth Speaker** via webapp

**Total Assembly Time:** 2-3 hours

---

## ğŸ“š ISL Datasets

If you don't want to collect your own data, use these public datasets:

| Dataset | Signs | Samples | Link |
|---------|-------|---------|------|
| **Mendeley ISL** | 26 alphabets | 52,000+ | [Mendeley](https://data.mendeley.com/datasets/n34wm8sb3x/1) |
| **IEEE DataPort ISL Fingerspelling** | 35 fingerspelling | 20,000+ | [IEEE](https://ieee-dataport.org/documents/isl-fingerspelling-image-dataset) |
| **RealSign ISL** | Multiple words | 1000+ videos | [GitHub](https://github.com/RealSign62/RealSign-Indian-Sign-Language-Dataset) |
| **Kaggle ISL Landmarks** | Multiple | Pre-extracted | [Kaggle](https://www.kaggle.com/datasets/eraakash/indian-sign-language-hand-landmarks-dataset) |
| **IIITA-ROBITA** | Research dataset | Various | [ROBITA](https://robita.iiita.ac.in/dataset.php) |

**Note:** These datasets may need preprocessing to extract MediaPipe landmarks. Use `collect_data.py` for best results.

---

## ğŸ“ Training Your Own Model

See detailed guide: [docs/ML_TRAINING_GUIDE.md](docs/ML_TRAINING_GUIDE.md)

**Tips for Better Accuracy:**

1. **Collect More Data** (300-500 samples per sign)
2. **Vary Conditions**:
   - Different lighting (bright, dim, outdoor)
   - Different backgrounds
   - Different hand positions/angles
   - Different distances from camera
3. **Balance Classes** (equal samples per sign)
4. **Use Data Augmentation** (rotate, flip, scale landmarks)
5. **Tune Hyperparameters**:
   - Adjust confidence threshold (0.6-0.8)
   - Adjust stability frames (10-20)
   - Try different network architectures

**Model Performance Benchmarks:**

| Hardware | Inference Time | FPS | Power |
|----------|----------------|-----|-------|
| Desktop (CPU) | ~15ms | 60+ | N/A |
| Raspberry Pi 4 | ~40ms | 25 | 3-5W |
| Pi Zero 2W (TFLite) | ~60ms | 15-18 | 1-2W |

---

## ğŸ“± Mobile Web App

Access from your phone on the same WiFi network as the Pi:

```
http://raspberrypi.local:5000
```

Or use the Pi's IP address:

```
http://192.168.1.xxx:5000
```

### Features:

- **Dashboard**: View status, current detected sign, FPS, start/stop inference
- **Bluetooth**: Scan, pair, connect Bluetooth speaker (no command line!)
- **Settings**: Adjust confidence, stability, cooldown, speech rate/volume
- **Collect**: Start/stop data collection remotely
- **Train**: Trigger model training, monitor progress
- **Logs**: View live system logs

See full guide: [docs/WEBAPP_GUIDE.md](docs/WEBAPP_GUIDE.md)

---

## ğŸš€ Deployment

### Raspberry Pi Setup

1. **Flash Raspberry Pi OS Lite** to microSD (64-bit recommended)
2. **Enable SSH** and WiFi (via `boot` partition configs)
3. **Boot Pi** and SSH in: `ssh pi@raspberrypi.local`
4. **Clone repository**:
   ```bash
   git clone https://github.com/Tusharjain-19/Smart_Glasses.git
   cd Smart_Glasses
   ```
5. **Run setup script**:
   ```bash
   chmod +x scripts/setup_pi.sh
   sudo ./scripts/setup_pi.sh
   ```
   This installs all dependencies, sets up services, configures Bluetooth/audio.

6. **Copy trained models** to Pi:
   ```bash
   scp models/* pi@raspberrypi.local:~/Smart_Glasses/models/
   ```

7. **Start web app**:
   ```bash
   sudo systemctl start smart-glasses-webapp
   ```

8. **Access webapp from phone**: `http://raspberrypi.local:5000`

9. **Pair Bluetooth speaker** via webapp

10. **Start inference** via webapp or:
    ```bash
    python src/deploy_pi.py
    ```

### Auto-Start on Boot

Enable services to start automatically:

```bash
sudo systemctl enable smart-glasses-webapp.service
sudo systemctl enable smart-glasses.service
```

---

## ğŸ—ºï¸ Roadmap

### Phase 1: Alphabets âœ… (Current)
- âœ… A-Z ISL fingerspelling recognition
- âœ… Desktop inference with TTS
- âœ… Basic model training

### Phase 2: Numbers ğŸ”„ (In Progress)
- ğŸ”„ 0-9 digit recognition
- ğŸ”„ Combined alphabet + number model

### Phase 3: Words ğŸ“‹ (Planned)
- Use LSTM for temporal sequence recognition
- Common ISL words ("Hello", "Thanks", "Sorry", etc.)
- Larger vocabulary (100-200 words)

### Phase 4: Sentences ğŸš€ (Future)
- Multi-word sentence recognition
- Grammar rules for ISL
- Context-aware predictions

### Phase 5: TFLite Optimization âš¡ (Ongoing)
- âœ… Basic TFLite conversion
- Model quantization (INT8)
- Further latency reduction (<30ms)

### Phase 6: 3D Printed Frame ğŸ–¨ï¸ (Ready)
- âœ… Complete design specifications
- Print and assemble
- Iterate on ergonomics

### Phase 7: Mobile Control âœ… (Complete)
- âœ… Flask web app
- âœ… Bluetooth pairing via webapp
- âœ… Settings management
- âœ… Remote training trigger

### Phase 8: Cloud Sync â˜ï¸ (Future)
- Cloud model training
- Shared sign database
- Multi-user learning

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/amazing-feature`
3. **Commit changes**: `git commit -m 'Add amazing feature'`
4. **Push to branch**: `git push origin feature/amazing-feature`
5. **Open a Pull Request**

### Areas for Contribution:

- ğŸ—ƒï¸ **ISL Datasets**: Collect and share more ISL sign data
- ğŸ§  **Model Improvements**: Better architectures, accuracy
- ğŸ¨ **UI/UX**: Improve web app interface
- ğŸ“– **Documentation**: Tutorials, translations
- ğŸ”§ **Hardware**: Alternative component recommendations
- ğŸ› **Bug Fixes**: Report and fix issues

### Code Style:

- Follow PEP 8 for Python
- Use meaningful variable names
- Add docstrings to functions
- Comment complex logic

---

## ğŸ“š References

This project builds upon excellent prior work:

- **MediaPipe Hands**: [Google MediaPipe](https://google.github.io/mediapipe/solutions/hands)
- **Sign Language Detection**: [dishak23/SignLanguage-Detection](https://github.com/dishak23/SignLanguage-Detection)
- **Hand Gesture Recognition**: [kinivi/hand-gesture-recognition-mediapipe](https://github.com/kinivi/hand-gesture-recognition-mediapipe)
- **Raspberry Pi SLT Glasses**: [Research project on wearable SLT](https://www.researchgate.net)
- **ISL Research**: Indian Sign Language Research and Training Centre (ISLRTC)

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ’¬ Support

- **Issues**: [GitHub Issues](https://github.com/Tusharjain-19/Smart_Glasses/issues)
- **Discussions**: [GitHub Discussions](https://github.com/Tusharjain-19/Smart_Glasses/discussions)
- **Email**: tusharjain19@example.com

---

## ğŸŒŸ Acknowledgments

- Google MediaPipe team for excellent hand tracking
- Indian Sign Language community for inspiration
- Open source contributors worldwide
- Raspberry Pi Foundation

---

**Made with â¤ï¸ for the deaf and hard-of-hearing community**

_Empowering communication through technology_ ğŸ¤Ÿ